## 0.  Adaboost、GBDT、XGBoost的区别？
* Adaboost强调自适应，GBDT强调减小残差
* Adaboost通过错误率来更新样本权重，来达到自适应的目的
* GBDT每一轮训练是为了减少上一轮的残差，进而在残差减少的方向即负梯度方向上建立新树
* XGBoost是GBDT的优化，和GBDT主要是目标函数上的区别
* XGBoost可以选择平方损失函数、交叉熵损失函数、hinge损失函数以及自定义损失函数
## 1.  GBDT和RF有哪些区别？
* 随机森林是bagging的思想，而GBDT是boosting的思想
* bagging降低方差，boosting降低偏差
## 2.  MySQL表的内连接、左连接和右连接的区别？
* 内连接：组合两个表中的记录，返回关联字段相符的记录，也就是返回两个表的交集部分
* 左连接：左表(a_table)的记录将会全部表示出来，而右表(b_table)只会显示符合搜索条件的记录，右表记录不足的地方均为NULL
* 右连接：左表(a_table)只会显示符合搜索条件的记录，而右表(b_table)的记录将会全部表示出来，左表记录不足的地方均为NULL
## 3.  MySQL查询第N大的数据？
## 4.  线性回归、逻辑回归、感知机、SVM的损失函数、优化方法分别是什么？
||损失函数|优化方法|
|:-:|:-:|:-:|
|线性回归|$\sum_{i=1}^{n}(y_i-\hat{y_i})^2$||
|LR|$-\frac{1}{m}\sum_{i=1}^{m}(y_i*logh_\theta(x_i)+(1-y_i)*log(1-h_\theta(x_i)))$||
|感知机|||
|SVM|||
## 5.  集成学习怎么选择弱学习器？是随机选择吗？
* 要根据目标问题的特征，看需要关注方差还是偏差，采用某个弱学习器进行预拟合，选取目标问题效果好的弱学习器来构建集成学习
## 6.  SVM、CART、KNN、GLM主要是减小方差还是偏差？
*	偏差描述的是估计值的期望与真实值之间的差距
*	方差描述的是估计值的变化范围、离散程度
*	在机器学习问题中，偏差与假设相关，方差与模型相关；换言之，偏差是指通过更简单的模型逼近现实生活中可能非常复杂的问题而引入的误差，方差与模型对馈送训练数据中可能存在的微小变化的过度敏感性有关
*	通常，参数算法具有高偏差，这使得它们更容易理解，但不太灵活，而具有很大灵活性的非参数机器学习算法具有高方差
||偏差|方差|
|:-:|:-:|:-:|
|SVM|低|高|
|CART|低|高|
|KNN|低|高|
|GLM|高|低|
|LR|高|低|
|LDA|高|低|
## 7.  字典中对key的要求？
* 必须有\_\_hash\_\_方法；Python中除了list、dict、set和内部至少带有上述三种类型之一的tuple之外，其余的对象都能当key
## 8.   ID3、C4.5怎么做分类？CART决策树怎么做分类？怎么做回归？
 * ID3使用信息增益最大化，C4.5使用信息增益比最大化，CART分类树使用基尼指数最大化原则建树，CART回归书的划分节点方式类似“平方误差”
## 9.  感知机和支持向量机的区别？
虽然感知机和SVM都是基于监督学习的分类器，但还是存在许多不同之处：
* 感知机追求最大程度正确划分、最小化错误，容易过拟合
* SVM追求大致正确分类的同时，避免过拟合
* 支持向量机寻找出的超平面唯一且最优
* 感知机是根据误分类点定义出的代价函数，求得的超平面并不唯一
* 感知机的学习策略是梯度下降法，SVM采用的是拉格朗日法
## 10.  L1和L2的区别?
* L1是模型各个参数绝对值之和，L2是模型各个参数平方和开根号
* L1趋向产生稀疏矩阵，L2趋向选择更多特征，这些特征会接近0，L2主要防止过拟合
* L1范数符合拉普拉斯分布，不是完全可微的，表现在图像上会有很多角出现，这些角和目标函数的接触机会远大于其它部分，就会造成最优值出现在坐标轴上，产生稀疏矩阵
* L2范数符合高斯分布，是完全可微的，图像上的棱角圆滑很多，一般最优值不会在坐标轴上出现

## 11.  逻辑回归的激活函数？

* LR的激活函数是sigmoid函数，$sigma(z)=\frac{1}{1+e^{-z}}$，它有很多优点：
  * 值域在0-1之间
  * 函数具有非常好的对称性、连续性
* 但并不是因为sigmoid拥有这些优良性质LR才使用sigmoid模型，而是推导出来的
## 12.  协同过滤？
* 基于用户的协同过滤
  * 核心：协同过滤分析用户兴趣，在用户群中找到指定用户的相似（兴趣）用户，综合这些相似用户对某一信息的评价，形成系统对该指定用户对此信息的喜好程度预测
  * 步骤a：找到和目标用户兴趣相似的用户集合
  * 步骤b：找到这个集合中用户喜欢的，且目标用户没有接触过的产品推荐给目标用户
* 基于产品的协同过滤
  * 核心：给用户推荐和他们之前喜欢的物品相似的物品
  * 步骤a：计算物品间的相似度
  * 步骤b： 根据物品相似度和用户的历史行为给用户生成推荐列表
## 13.  FM模型？
## 14.  TF-IDF？
* TF：词频，表示词条在文本中出现的频率；IDF：逆向文件频率，总文档数目/包含某词条的文档数目的结果取对数；TF-IDF=TF*IDF
* 主要思想：如果某个词或短语在一篇文章中出现的频率高（即TF高），并且在其他文章中很少出现（即IDF高），则认为此词或者短语具有很好的类别区分能力，适合用来分类
* 即：TF刻画了词语t对某篇文档的重要性，IDF刻画了词语t对整个文档集的重要性
## 15.  Hive内部表和外部表？
* 删除内部表，就删除了元数据和数据；删除外部表，只删除元数据，不删除数据
* 大多数情况下，内部表和外部表的区别不大，如果所有数据都在Hive中运行，倾向于选择内部表，但如果是Hive和其他工具要针对相同的数据进行处理，外部表更合适，使用外部表访问存储在HDFS上的初始数据，然后通过Hive转换数据并存储到内部表中，使用外部表的场景可以抽象为针对一个数据集有多个Schema
* 通过外部表和内部表的比较，Hive其实就是针对存储在HDFS上的数据提供了一种新的抽象，而不是管理存储在HDFS上的数据
## 16.  Hive分区表和分桶表?
* Hive数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更加高效，同时表和分区也可以进一步划分为桶(Buckets)，分桶表的原理和MapReduce编程中的HashPartitioner原理类似
* 分区和分桶都是细化数据管理，但分区表是手动添加分区，由于Hive是读模式，所以对添加进分区的数据不做模式校验，分桶表中的数据是按照某些分桶字段进行hash散列所形成的多个文件，数据的准确性高很多。hash就是找到一种数据内容和数据存放地址之间的映射关系
## 17.  试实现新浪微博的用户推荐？
## 18.  梯度下降算法有哪些？

* 梯度下降算法有很多，大致可以分为三部分：BGD批量梯度下降、SGD随机梯度下降和MBGD小批量梯度下降

## 19.  梯度下降怎么解决局部最优问题？

* <https://ruder.io/optimizing-gradient-descent/index.html#momentum>

## 20.  牛顿法与梯度下降的比较？

* 梯度下降法求解目标函数的极值（每次根据梯度方向前进一定的步长）
* 牛顿法则变相的通过求解目标函数一阶导为零的参数值来寻找极值
* 梯度下降法之需要计算一阶导数，牛顿法需要计算Hessian矩阵，牛顿法时间复杂度高
* 梯度下降收敛慢，可能陷入Z字形轨迹，且靠近极小值的时候收敛速度明显下降
* 牛顿法收敛快，但每一步都需要计算Hessian矩阵，由于Hessian矩阵在迭代中不断减小，牛顿法的步长会逐渐缩小，在非凸问题中就容易陷入鞍点，因此牛顿法对初始值更敏感
* 梯度下降适合特征纬度较大的场景，牛顿法相反

## 21.  RF原理？

* 随机森林是一种特殊的bagging方法，它将决策树用于bagging中的模型
* 首先使用bootstrap方法生成m个训练集，将会有36.8%的数据没被选中
* 然后对每一个训练集构造一棵决策树，但是在寻找最优分隔点的时候并不是对所有特征寻找某指标（如信息增益）最大的，而是在特征中随机抽取一部分特征来寻找节点进行分裂
* 最后进行预测，就是使用bagging的预测思想，分类问题进行投票，回归问题取平均
* 随机森林有集成的思想，实际上随机表现在样本随机和属性随机两部分上，可避免过拟合

## 22.  RF在特征筛选中的应用？

* 随机森林做特征筛选比较简单，就是通过计算特征重要度，主要是看每个特征在随机森林中的每棵树上做了多大的贡献，然后取平均值，最后比较不同特征之间的贡献大小
* 关于随机森林计算特征重要性：
  1. 对于随机森林中的每一棵决策树，使用相应的OBB（袋外数据）来计算它的袋外数据误差，记为errOBB1
  2. 随机的对OBB的所有样本特征X加入噪声干扰，再次计算袋外数据误差，记为errOBB2
  3. 假设随机森林中有N棵树，那么特征X的重要性就可表示为$\frac{\sum(errOBB2-errOBB1)}{N}$
  4. 该表达式可以理解为：若给某个特征随机加入噪声之后,袋外的准确率大幅度降低,则说明这个特征对于样本的分类结果影响很大,也就是说它的重要程度比较高
* 关于随机森林做特征筛选（假设我们想从N个原始特征中选取m个特征）：
  1. 对随机森林的特征变量按照重要度降序排序
  2. 从当前特征集中删除一定比例不重要的特征，得到新特征集
  3. 用新的特征集计算特征重要性
  4. 重复以上步骤直到剩下m个特征
## 23.  如何处理数据不均衡？

* **从数据角度**

  * 主动获取
  
  * 
    针对少量样本数据，可以尽可能去扩大这些少量样本的数据集，或者尽可能去增加他们特有的特征来丰富数据的多样性
    
  * 算法采样：上采样、下采样、生成合成数据
  
    1. ADASYN采样：
  
       ​	ADASYN为样本较少的类生成合成数据，其生成的数据与更容易学习的样本相比，更难学习。基本思想是根据学习难度的不同，对不同的少数类的样本使用加权分布。其中，更难学习的少数类的样本比那些更容易学习的少数类的样本要产生更多的合成数据。因此，ADASYN方法通过以下两种方式改善了数据分布的学习：(1)减少由于类别不平衡带来的偏差；(2)自适应地将分类决策边界转移到困难的例子
  
    2. SMOTE采样：
  
       ​	从少数类创建新的合成点，以增加其基数。但是SMOTE算法也有一定的局限性。具体有两项，一是在近邻选择时，存在一定的盲目性。在算法执行过程中，需要确定Ｋ值，即选择几个近邻样本，这个需要根据具体的实验数据和实验人自己解决。二是该算法无法克服非平衡数据集的数据分布问题，容易产生分布边缘化的问题。由于负类样本的分布决定了其可选择的近邻，如果一个负类样本处在负类样本的边缘，则由此负类样本和近邻样本产生的样本也会处在边缘，从而无法确定正负类的分类边界
    
  * 数据增强，加噪音增强模型鲁棒性
  
  * 改变权重：设定惩罚因子，如libsvm等算法里设置的正负样本的权重项等。惩罚多样本类别，也可以加权少样本类别 
  
* **从模型评估的角度**

  * **谨慎选择AUC作为评价指标**：对于数据极端不平衡问题，可以观察不同算法在同一份数据下的训练结果的precision和recall，这样做有两个好处，一是可以了解不同算法对于数据的敏感程度，二是可以明确采取哪种评价指标更合适。针对机器学习中的数据不平衡问题，建议更多PR(Precision-Recall曲线)，而非ROC曲线，具体原因画图即可得知，如果采用ROC曲线来作为评价指标，很容易因为AUC值高而忽略实际对少两样本的效果其实并不理想的情况
  * **不要只看Accuracy**：Accuracy可以说是最模糊的一个指标了，因为这个指标高可能压根就不能代表业务的效果好，在实际生产中，我们可能更关注precision/recall/mAP等具体的指标，具体侧重那个指标，得结合实际情况看

* **从算法角度**

  * 选择对数据倾斜相对不敏感的算法，如树模型等
  * 使用集成学习的思想，例如负样本个数5000，正样本个数100000，将正样本随机抽取50000个，并且分成10等份，负样本与每一份正样本组成1:1的10个训练集，训练10个模型进行融合
  * 将任务转换成**异常检测**问题。譬如有这样一个项目，需要从高压线的航拍图片中，将松动的螺丝/零件判断为待检测站点，即负样本，其他作为正样本，这样来看，数据倾斜是非常严重的，而且在图像质量一般的情况下小物体检测的难度较大，所以不如将其转换为无监督的异常检测算法，不用过多的去考虑将数据转换为平衡问题来解决

## 24.  欠采样和过采样的缺点是什么？优化方法是什么？

* 欠采样可能丢失大部分原始数据信息
* 过采样可能破坏原始数据的分布特征
* 优化方法参考Q23，可以从模型评估、算法角度考虑优化方法

## 25.  梯度下降中的动量（Momentum）？

* 梯度下降在靠近极小点的时候存在震荡的问题
* 动量在物理学中用来解释一段时间内作用所产生的物理量，梯度下降中的动量我们可以理解为“惯性”
* 使用动量时，参数更新在基础梯度下降的基础上，考虑上次参数更新的更新量乘上一个[0, 1]之间的因子
* 梯度下降使用动量后，主要有以下两点优势：
  1. 加速收敛（如果此时更新动量的方向与上一时刻相同，则会加速，反之减速）
  2. 提高精度（减少收敛过程中的震荡）

## 26.  ROC曲线制作过程？AUC？PR曲线？

## 27.  dropout有什么用？

* 典型的神经网络训练过程就是将输入通过网络进行正向传导，然后将误差进行反向传播，dropout就是针对这一过程，随机的删除隐藏层的部分单元，进行网络训练
* 在训练阶段激活dropout，测试阶段将dropout屏蔽掉
* dropout会使神经网络具有冗余表示，在每轮迭代中，总是随机的屏蔽掉一定比例的神经元，输出y并不知道它在使用哪些特征，相比较于没有dropout过程（没有dropout会使得模型专注于某些特征），现在的模型将注意力分散到每个特征上，原本权重很高的特征被压缩，达到一定正则化效果
* dropout在一定程度上相当于集成，它针对每一个batch的数据进行操作，每次都生成一个小模型，最终取平均
* dropout会降低复杂依赖性，由于每次权值更新的时候，隐藏层的节点都是以一定概率随机出现，不能保证某些节点每次都同时出现，这样就减少了神经元之间的依赖性，减少神经元之间的复杂依赖性


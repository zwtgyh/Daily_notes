## 0.  Adaboost、GBDT、XGBoost的区别？
	* Adaboost强调自适应，GBDT强调减小残差
	* Adaboost通过错误率来更新样本权重，来达到自适应的目的
	* GBDT每一轮训练是为了减少上一轮的残差，进而在残差减少的方向即负梯度方向上建立新树
	* XGBoost是GBDT的优化，和GBDT主要是目标函数上的区别
	* XGBoost可以选择平方损失函数、交叉熵损失函数、hinge损失函数以及自定义损失函数。
## 1.  GBDT和RF有哪些区别？
## 2.  MySQL表的内连接、左连接和右连接的区别？
	* 内连接：组合两个表中的记录，返回关联字段相符的记录，也就是返回两个表的交集部分
	* 左连接：左表(a_table)的记录将会全部表示出来，而右表(b_table)只会显示符合搜索条件的记录。右表记录不足的地方均为NULL
	* 右连接：左表(a_table)只会显示符合搜索条件的记录，而右表(b_table)的记录将会全部表示出来。左表记录不足的地方均为NULL
## 3.  MySQL查询第N大的数据？
## 4.  线性回归、逻辑回归、感知机、SVM的损失函数、优化方法分别是什么？
## 5.  集成学习怎么选择弱学习器？是随机选择吗？
	* 要根据目标问题的特征，看需要关注方差还是偏差，采用某个弱学习器进行预拟合，选取目标问题效果好的弱学习器来构建集成学习
## 6.  SVM、CART、KNN、GLM主要是减小方差还是偏差？
## 7.  字典中对key的要求？
	* 必须有__hash__方法；Python中除了list、dict、set和内部至少带有上述三种类型之一的tuple之外，其余的对象都能当key
## 8.  ID3、C4.5怎么做分类？CART决策树怎么做分类？怎么做回归？
	* ID3使用信息增益最大化，C4.5使用信息增益比最大化，CART分类树使用基尼指数最大化原则建树，CART回归书的划分节点方式类似“平方误差”
## 9.  感知机和支持向量机的区别？
	* 支持向量机寻找出的超平面唯一且最优，而感知机是根据误分类点定义出的代价函数求得的超平面不唯一，包含多个
## 10.  L1和L2的区别?
## 11.  逻辑回归的激活函数？
## 12.  协同过滤？
	* 协同过滤分析用户兴趣，在用户群中找到指定用户的相似（兴趣）用户，综合这些相似用户对某一信息的评价，形成系统对该指定用户对此信息的喜好程度预测
## 13.  FM模型？
## 14.  TF-IDF？
## 15.  Hive内部表和外部表？
	* 删除内部表，就删除了元数据和数据；删除外部表，只删除元数据，不删除数据
	* 大多数情况下，内部表和外部表的区别不大，如果所有数据都在Hive中运行，倾向于选择内部表，但如果是Hive和其他工具要针对相同的数据进行处理，外部表更合适，使用外部表访问存储在HDFS上的初始数据，然后通过Hive转换数据并存储到内部表中，使用外部表的场景可以抽象为针对一个数据集有多个Schema
	* 通过外部表和内部表的比较，Hive其实就是针对存储在HDFS上的数据提供了一种新的抽象，而不是管理存储在HDFS上的数据
## 16.  Hive分区表和分桶表?
	* Hive数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更加高效，同时表和分区也可以进一步划分为桶(Buckets)，分桶表的原理和MapReduce编程中的HashPartitioner原理类似
	* 分区和分桶都是细化数据管理，但分区表是手动添加分区，由于Hive是读模式，所以对添加进分区的数据不做模式校验，分桶表中的数据是按照某些分桶字段进行hash散列所形成的多个文件，数据的准确性高很多。hash就是找到一种数据内容和数据存放地址之间的映射关系
## 17.  试实现新浪微博的用户推荐？
## 18.  梯度下降算法有哪些？
## 19.  梯度下降怎么解决局部最优问题？
## 20.  牛顿法在梯度下降中的应用？
## 21.  RF原理？
## 22.  RF在特征筛选中的应用？
## 23.  数据清洗中怎么处理数据不均衡？
## 24.  欠采样和过采样的缺点是什么？优化方法是什么？
## 25.  梯度下降中的动量？
## 26.  ROC曲线制作过程？AUC？PR曲线？